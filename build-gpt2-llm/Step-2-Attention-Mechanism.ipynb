{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f60c1ed-2f27-4790-b06c-3aed71c1b144",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e6b0ad-51bc-40ce-b88b-5fa072a7a2a1",
   "metadata": {},
   "source": [
    "## A Simple Self-Attention without Trainable Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5213b3-a45f-4d64-b2cd-ab85519f71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # Your\n",
    "        [0.55, 0.87, 0.66],  # journey\n",
    "        [0.57, 0.85, 0.64],  # starts\n",
    "        [0.22, 0.58, 0.33],  # with\n",
    "        [0.77, 0.25, 0.10],  # one\n",
    "        [0.05, 0.80, 0.55]   # step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac26fbb-4665-494a-a1d1-f1cfb9aa395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ad74e8-108b-43e3-ad5f-db4ee50b21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "# Normalize attention_scores\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(f\"Attention Weights: {attn_weights_2_tmp}\")\n",
    "print(f\"Sum: {attn_weights_2_tmp.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c7758e6-d969-4cf1-ba03-351a2bd03e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Softmax for normalizatiob\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(f\"Attention weights: {attn_weights_2_naive}\")\n",
    "print(f\"Sum: {attn_weights_2_naive.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c745d2-e5e7-4fd5-ae3a-d4698e097000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(f\"Attention weights: {attn_weights_2}\")\n",
    "print(f\"Sum: {attn_weights_2.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26dc1f8c-d941-433f-9fe9-8e1c506d2b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f2a8401-64b5-46ff-bf1d-fbb55bd93363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Calculating attention weights for all Input tokens\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c42b691-aef1-4384-92c3-2ca419241a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Using Matrix Multiplication instead of for loop\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c26897-50a6-4b20-a411-0ff1dd584c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78eb2ea7-3ba8-48fa-9ee2-8aadd5dfb845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "row_2_sum = sum(attn_weights[1][:])\n",
    "print(f\"Row 2 sum: {row_2_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f827b4-cbd8-4192-bae0-901e4899b312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Compute all context vectors\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9542b1-91a3-49f0-ad81-e3c5d04dfde3",
   "metadata": {},
   "source": [
    "## Self-Attention with Trainable Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2ad2c5f-8e14-47b9-9219-12356cde1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d23a3653-ba5e-4c7b-ab95-a912256d9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the three weight matrices W_q, W_k, W_v\n",
    "torch.manual_seed(314159)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68fe78ef-7820-44c7-b8b9-f3e7ed1c3b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5779, 1.3467])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57ed3a49-0c68-40c2-8655-bfa623f3d423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(f\"keys.shape: {keys.shape}\")\n",
    "print(f\"values.shape: {values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54339d28-7d14-4224-a221-125d5ca7ad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5010)\n"
     ]
    }
   ],
   "source": [
    "# Compute Attention Scores\n",
    "keys_2 = keys[1]\n",
    "attn_scores_22 = query_2.dot(keys_2)\n",
    "print(attn_scores_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13a1cc07-73b4-4029-83f3-df612519e978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2540, 1.5010, 1.4686, 0.8360, 0.4714, 1.2046])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2373105f-e083-47b8-8ff7-4597fadf1087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1773, 0.2112, 0.2064, 0.1319, 0.1020, 0.1712])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f26d3470-8af5-4907-aba0-ccdfb8f0160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7261, 0.8764])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "320643eb-fbb4-4ffb-b1ad-3463b142d7a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7216, 0.8740],\n",
      "        [0.7261, 0.8764],\n",
      "        [0.7259, 0.8762],\n",
      "        [0.7013, 0.8581],\n",
      "        [0.7083, 0.8631],\n",
      "        [0.7056, 0.8611]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Using Simple Attention mechanism\n",
    "from attention import SelfAttention_v1\n",
    "\n",
    "torch.manual_seed(314159)\n",
    "sa_v1 = SelfAttention_v1(d_in=d_in, d_out=d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a79add2e-ba50-4c6f-9224-252ca433a822",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1866,  0.1720],\n",
      "        [-0.1893,  0.1666],\n",
      "        [-0.1893,  0.1666],\n",
      "        [-0.1900,  0.1656],\n",
      "        [-0.1904,  0.1676],\n",
      "        [-0.1895,  0.1652]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Using Simple Attention mechanism\n",
    "from attention import SelfAttention_v2\n",
    "\n",
    "torch.manual_seed(314159)\n",
    "sa_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d416b-e0fb-439b-b6bd-a83d048d5ae7",
   "metadata": {},
   "source": [
    "## Causal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c84e878b-56a8-44f8-b6d1-2431e9707b7e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1525, 0.1755, 0.1751, 0.1659, 0.1602, 0.1708],\n",
      "        [0.1658, 0.1673, 0.1671, 0.1671, 0.1640, 0.1687],\n",
      "        [0.1658, 0.1673, 0.1671, 0.1671, 0.1642, 0.1685],\n",
      "        [0.1685, 0.1656, 0.1656, 0.1672, 0.1655, 0.1676],\n",
      "        [0.1657, 0.1672, 0.1673, 0.1662, 0.1682, 0.1655],\n",
      "        [0.1685, 0.1657, 0.1655, 0.1676, 0.1636, 0.1691]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3192f071-9902-4a1c-96ad-ab0ce95535ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a mask\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea93558d-7004-40ee-b7ff-6f00f35cf846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1525, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1658, 0.1673, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1658, 0.1673, 0.1671, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1685, 0.1656, 0.1656, 0.1672, 0.0000, 0.0000],\n",
      "        [0.1657, 0.1672, 0.1673, 0.1662, 0.1682, 0.0000],\n",
      "        [0.1685, 0.1657, 0.1655, 0.1676, 0.1636, 0.1691]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply the mask for the attention weights\n",
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ac7e9ca-d957-49ca-af7e-9cc4aba3b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4978, 0.5022, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3315, 0.3344, 0.3341, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2527, 0.2484, 0.2483, 0.2507, 0.0000, 0.0000],\n",
      "        [0.1985, 0.2003, 0.2004, 0.1992, 0.2016, 0.0000],\n",
      "        [0.1685, 0.1657, 0.1655, 0.1676, 0.1636, 0.1691]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Renormalize the attention weights\n",
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8049e981-07b0-4dbc-b2a8-e6566a03e058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0287,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0007,  0.0131,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0008,  0.0134,  0.0122,    -inf,    -inf,    -inf],\n",
      "        [-0.0045, -0.0288, -0.0294, -0.0159,    -inf,    -inf],\n",
      "        [ 0.0028,  0.0154,  0.0161,  0.0072,  0.0243,    -inf],\n",
      "        [-0.0054, -0.0294, -0.0308, -0.0135, -0.0478, -0.0009]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# A more effective masking\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b01c6fdb-c34e-488b-8073-9816b7682a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4978, 0.5022, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3315, 0.3344, 0.3341, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2527, 0.2484, 0.2483, 0.2507, 0.0000, 0.0000],\n",
      "        [0.1985, 0.2003, 0.2004, 0.1992, 0.2016, 0.0000],\n",
      "        [0.1685, 0.1657, 0.1655, 0.1676, 0.1636, 0.1691]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3786e76a-a603-4882-b35e-57b4e15bf62c",
   "metadata": {},
   "source": [
    "### Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df8814a1-9897-4e74-a06a-1edb70c64c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 2., 2., 0.],\n",
      "        [2., 2., 2., 0., 2., 2.],\n",
      "        [2., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2.],\n",
      "        [0., 0., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(314159)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dde6ba40-03f0-4f9b-bb75-81ff8a0f0fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9956, 1.0044, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6629, 0.0000, 0.6683, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5054, 0.4967, 0.4966, 0.5013, 0.0000, 0.0000],\n",
      "        [0.3971, 0.4006, 0.4008, 0.3983, 0.4032, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3311, 0.3351, 0.3271, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply dropout to attention weights\n",
    "torch.manual_seed(314159)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b642a8b3-232a-41b2-b65e-52b823a5dfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Create batch\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65bb0fb2-62d1-42ad-b226-dd7bb1b5c5cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Verify Causal Attention implementation\n",
    "from attention import CausalAttention\n",
    "\n",
    "torch.manual_seed(314159)\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "ca = CausalAttention(d_in=d_in, d_out=d_out, context_length=context_length, dropout=0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(f\"context_vecs.shape: {context_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a330168-eab8-4b9c-be2c-d6bbbc601539",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Extending Single-Head Attention to Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b88d2da-1e6e-467f-ae1d-27d9af7ba73b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3760, -0.1756, -0.3800,  0.1722],\n",
      "         [-0.2934,  0.0484, -0.3221,  0.1930],\n",
      "         [-0.2686,  0.1255, -0.3025,  0.1965],\n",
      "         [-0.2149,  0.1355, -0.2565,  0.1813],\n",
      "         [-0.2300,  0.1815, -0.2340,  0.1389],\n",
      "         [-0.1895,  0.1652, -0.2215,  0.1593]],\n",
      "\n",
      "        [[-0.3760, -0.1756, -0.3800,  0.1722],\n",
      "         [-0.2934,  0.0484, -0.3221,  0.1930],\n",
      "         [-0.2686,  0.1255, -0.3025,  0.1965],\n",
      "         [-0.2149,  0.1355, -0.2565,  0.1813],\n",
      "         [-0.2300,  0.1815, -0.2340,  0.1389],\n",
      "         [-0.1895,  0.1652, -0.2215,  0.1593]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# Verify the use of a list of Causal Attentions as Multi-head attention\n",
    "# In this case the Causal Attention will be processed sequentially\n",
    "from attention import MultiHeadAttentionWrapper\n",
    "\n",
    "torch.manual_seed(314159)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in=d_in, d_out=d_out, context_length=context_length, dropout=0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(f\"context_vecs.shape: {context_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4997ea6a-2fad-4651-b0a5-ccf6014f048e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.6960, -0.2379],\n",
      "         [-0.7365, -0.2963],\n",
      "         [-0.7514, -0.3150],\n",
      "         [-0.7396, -0.3368],\n",
      "         [-0.7563, -0.3366],\n",
      "         [-0.7409, -0.3497]],\n",
      "\n",
      "        [[-0.6960, -0.2379],\n",
      "         [-0.7365, -0.2963],\n",
      "         [-0.7514, -0.3150],\n",
      "         [-0.7396, -0.3368],\n",
      "         [-0.7563, -0.3366],\n",
      "         [-0.7409, -0.3497]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Verify MultiHeadAttention for parallel instead of sequential\n",
    "from attention import MultiHeadAttention\n",
    "\n",
    "torch.manual_seed(314159)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in=d_in, d_out=d_out, context_length=context_length, dropout=0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(f\"context_vecs.shape: {context_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bc9e46b-9a02-4ab9-af24-71adc6dcdcdc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize MultiHeadAttention for smallest GPT-2 model\n",
    "block_size = 1024\n",
    "d_in, d_out = 768, 768\n",
    "num_heads = 12\n",
    "mha = MultiHeadAttention(d_in=d_in, d_out=d_out, context_length=block_size, dropout=0.0, num_heads=num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8afc8-df6c-44a7-be6d-04302159d280",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
